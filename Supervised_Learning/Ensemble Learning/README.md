# Ensemble Learning
By: Channing Vernon

## Purpose:
---
I will be covering the two algorithm of ensemble learning called Bagging, and Random Forests on the "Diabetes" dataset from Kaggle.

## Overview of Ensemble Learning:
---
Ensemble Learning alludes to a gathering of models cooperating to take care of a typical issue. As opposed to relying upon a single model for the best solution, ensemble learning uses the benefits of a few distinct techniques to balance each model's singular shortcomings. The subsequent assortment ought to be less error induced than any model alone.

![alt text](https://cdn.educba.com/academy/wp-content/uploads/2019/12/ensemble-method-in-machine-learning.png)

## Pros and Cons
---
Pros
- Produces better predictions compared to a single model.

- Ensures the stability of the model.

- Ensemble methods are very useful when there is both linear and non-linear type of data in the dataset; different models can be combined to handle this type of data.

Cons
-  Any wrong selection can lead to lower predictive accuracy than an individual model.

- the output of the ensembled model is hard to predict and explain, making it less interpretable.


## Data: 
---
The diabetes data contains 768 rows, and 9 columns. Link to dataset is: [link](https://www.kaggle.com/datasets/mathchi/diabetes-data-set)

- Pregnancies
- Glucose
- BloodPressure
- SkinThickness
- Insulin
- BMI
- DiabetesPedigreeFunction
- Age
- Outcome

## Libraries:
---
- Numpy: [link](https://numpy.org)
- Pandas: [link](https://pandas.pydata.org)
- Scikit-learn: [link](https://scikit-learn.org/stable/)
- Seaborn: [link](https://seaborn.pydata.org)
- Matplotlib.pyplot: [link](https://matplotlib.org)
- Mlxtend.plotting[link](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/)
