{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "### What is Gradient Descent?\n",
    "- Gradient descent is an iterative optimization algorithm for finding the local minimum of a function.\n",
    "- To find the local minimum of a function using gradient descent, we must take steps proportional to the negative of the gradient (move away from the gradient) of the function at the current point. If we take steps proportional to the positive of the gradient (moving towards the gradient), we will approach a local maximum of the function, and the procedure is called Gradient Ascent.\n",
    "\n",
    "![alt text](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)\n",
    "\n",
    "### Learning Rate\n",
    "- If the learning rate is too high, we might OVERSHOOT the minima and keep bouncing, without reaching the minima.\n",
    "- If the learning rate is too small, the training might turn out to be too long.\n",
    "\n",
    "![alt text](https://editor.analyticsvidhya.com/uploads/43266images.png)\n",
    "- (a) This learning rate is good, our model convergres to the minimum.\n",
    "- (b) Our learning rate is too small meaning that it takes more time but eventually converges to the minimum.\n",
    "- (c) Our learning rate is higher than the optimal value so we are overshooting but it converges.\n",
    "- (d) Learning rate is very large, it overshoots and diverges, moves away from the minima, performance decreases on learning.\n",
    "\n",
    "![alt text](https://editor.analyticsvidhya.com/uploads/40982epochss.png)\n",
    "\n",
    "Now lets begin to imppliment this learning algorithm on some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
